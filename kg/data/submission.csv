ID,track_id,track name,title,authors,submitted,last updated,form fields,keywords,decision,abstract,url,doi,img
104,2,sem23-pd,A Toolset for Normative Interpretations in FLINT,"Thom van Gessel, Giulia Biagioni, Jeroen Breteler, Ioannis Tolios and Erik Boertjes",2023-07-05 09:53,2023-07-05 09:59,"(Publication Resources) Yes
(Code Availability) https://gitlab.com/normativesystems/knowledge-modeling/flint-ontology
https://gitlab.com/normativesystems/ui/interpretation-editor
(Data Availability) 
(Ontology Availability) 
(Demo link) https://choppr.app
https://norm-editor.tnodatalab.nl/","Norms
Normative systems
Legal interpretation",accept,"In light of the significant challenge posed by the ability to track and comprehend diverse interpretations of norms, this paper introduces a method for interpreting norms using ontologies and digital tools. We propose the FLINT ontology, a language for expressing interpretations of normative sources, as well as a set of software tools to support its utilization.

The FLINT ontology represents norms as normative acts with pre- and postconditions, providing a structured framework for interpreting regulations. Importantly, each interpretation is linked to its source, ensuring traceability and explainability. The software tools we introduce in this paper include a source decomposition application that converts normative documents into RDF-based representations, a norm editor for creating interpretations of norms based on FLINT, and the integration of an automated assistance module that generates annotation suggestions to assist users in encoding normative texts.",https://ceur-ws.org/Vol-3526/paper-03.pdf,,
264,1,sem23-research,Virtual Reality based Access to Knowledge graphs for History Research,"Julia Becker, Mario Botsch, Mohammad Fazleh Elahi, Melanie Derksen, Philipp Cimiano, Angelika Maier, Marius Maile, Ingo Oliver Pätzold, Jonas Penningroth, Bettina Reglin, Markus Rothgänger and Silke Schwandt",2023-05-17 10:46,2023-05-23 21:20,"(Paper Type) Full
(Student) 
(Publication Resources) No
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) ","knowledge graphs
virtual reality
digital humanities
digital history
linked data
semantic web",accept,"Purpose: Knowledge Graphs have so far been intensively used in the cultural heritage domain. Current interaction paradigms and interfaces however are often limited to textual representations or 2D visualizations, not taking into account the 4D nature of data. In digital history in particular, where events as well as geographical and temporal relationships play an important role, exploration paradigms that take into account the 4D nature of event-related data are important, as they have the potential to support historians in generating new knowledge and discovering new relationships. In this paper we explore the potential of Virtual Reality (VR) as a paradigm allowing digital humanities researchers, historians in particular, to explore from an egocentric perspective a semantic 4D space defined by knowledge graphs. Methodology: We present eTaRDiS: a VR-based tool supporting immersive exploration of knowledge graphs. We evaluate the tool in the context of a task in which historians and laypersons with a history background explore DBpedia and Wikidata. We report results of a study involving 14 subjects that interacted with the data in eTaRDiS in the context of a specific task, in order to gain insights regarding the interaction patterns of users with our system. For this, the interaction of the users within the application was recorded and transcribed. The usability of the tool was evaluated using a questionnaire. Findings: The usability evaluation showed that our tool achieved an overall System Usability Scale (SUS) score of 71.92, corresponding to a ‘satisfactory’ rating. While the mean score reached with laypersons with a history background was quite high with 76.0, corresponding to a rating of ‘excellent’, the score for historians was lower with 69.4, corresponding to a ‘sufficient to satisfactory’ rating. In the analysis of the recordings, we found that respondents quickly identified the relevant information in the tasks using a variety of strategies and taking advantage of the features provided in eTaRDiS. Value: eTaRDiS is to our knowledge the first VR-based exploration tool supporting the exploration of knowledge graphs. The findings of the usability evaluation and the qualitative analysis of exploration patterns show that the system could potentially be a valuable tool for allowing digital humanities researchers to explore knowledge graphs as a way to discover new relationships between historical events and persons of interest.",https://doi.org/10.3233/SSW230011,10.3233/SSW230011,https://2023-eu.semantics.cc/img/illustrations/264-Semantic23-Drawing-Text-01.png
319,4,sem23-industry,Linked Data Budgeting with the HTML Vocabulary,Wouter Beek and Flores Bakker,2023-06-13 21:24,1970-01-01 00:00,(Demo link) https://github.com/floresbakker/htmlvoc,"Linked Data Budgeting
HTML
Vocabulary
Information Landscape",accept,"The goal of linked data is to reach full coverage of the entire information landscape. Linked data should start at the data source, and run all the way up to human-readable documents. Current applications of linked data cover the first half of the information landscape: data description, data collection, data integration, and data publication. The second half of the information landscape remains mostly untouched: creating information products, using information products, and tracing information flows.

What is missing is significant linked data support for the second half of the information landscape. This presentation shows the HTML Vocabulary, a new vocabulary that integrates the most widely used formatting technique for human-consumable content with the linked data paradigm. We show how we create budget tables with Linked Data Business Rules (not code) that are part of the vocabulary.

In a budget table, a lot of meaning is encoded in how the table is structured and presented. The HTML Vocabulary allows us to capture structural meaning and presentational meaning in the paradigm of propositional meaning.

The Dutch Ministry of Finance is applying the Linked Data Budgeting approach to generate the National Budget of The Netherlands. We show how generating human-consumable budget tables as linked data allows us to trace information flows from a cell in a budget table, all the way back to the data source, thereby covering the full information landscape in linked data.
",,,
384,2,sem23-pd,Towards Assessing FAIR Research Software Best Practices in an Organization Using RDF-star,Ana Iglesias-Molina and Daniel Garijo,2023-07-04 18:36,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) https://github.com/oeg-upm/oeg-software-graph
(Data Availability) https://doi.org/10.5281/zenodo.8114677
(Ontology Availability) 
(Demo link) ","Research Software
Metadata
FAIR software
FAIR
RDF-star",accept,"An increasing number of scientists share the source code used or developed during their research (i.e., their research software) in open repositories, in order to support the results described in their publications. Recent best practices have been proposed by the community by aligning the Findable, Accessible, Interoperable and Reusable (FAIR) principles to Research Software. However, there are currently no means to assess the systematic adoption of these practices in a research organization. In this poster, we propose an automated pipeline to transform the software metadata of an organization as a Knowledge Graph in order to assess the current adoption of FAIR Research Software best practices. Our poster shows results from our own GitHub organization, the Ontology Engineering Group.",https://ceur-ws.org/Vol-3526/paper-09.pdf,,
460,4,sem23-industry,LD Wizard: Create Linked Data in One Spell,"Wouter Beek, Enno Meijers, Erwin Folmer, Ivo Zandhuis, Richard Zijdeman, Pieter van Everdingen and Mark Lindeman",2023-06-13 21:58,1970-01-01 00:00,(Demo link) https://github.com/pldn/LDWizard,"ETL
domain experts
GUI
open source",accept,"Contemporary approaches for creating linked data require (1) domain knowledge of the source data, (2) knowledge of linked data techniques, and (3) significant programming skills to implement the transformations. This makes creators of new linked data a rare breed: they must have 3 individually rare traits.

LD Wizard is a new GUI-based ETL application that lowers the threshold for becoming a successful linked data creator. LD Wizard supports domain exports that have only a little bit of knowledge about linked data techniques, and that do not need to have significant programming skills.

LD Wizard supports the transformation of tabular source data into linked data, by allowing the user to select terms (classes and properties) from their domain. Linked data that is created with LD Wizard can be downloaded as an RDF file, or can be uploaded to a triple store.

Since LD Wizard focusses on domain experts, it can be configured for specific domains. There are currently configurations for the following domains: cultural heritage, biology, social sciences, humanities, and geospatial.

In this presentation, we will show how you can use LD Wizard, and how you can configure one for your own domain.

The LD Wizard was created by the Dutch Digital Heritage Network (NDE), and is currently maintained as open source software by Platform Linked Data Netherlands (PLDN). LD Wizard is actively being developed by the linked data community, and there are open bounties for new contributors to pick up.",,,
589,4,sem23-industry,MALENA – New IFC’s virtual analyst supporting ESG due diligence,Carlos Arias,2023-05-10 21:27,1970-01-01 00:00,(Demo link) ,"Artificial Intelligence
ESG due diligence
NLP
Sentiment Analysis
Emerging Markets
Development Institutions",accept,"The International Finance Corporation (or IFC) is a member of the World Bank Group that works with the private sector to create markets and jobs for people in developing countries. We strive to unlock new, innovative opportunities for the communities in which we work, but we are also accountable to the people that are affected by the projects we finance. We are accountable to our partners, clients, and communities as we aim to achieve our development objectives in an environmentally and socially responsible manner.

MALENA (Machine Learning ESG Analyst) is an Artificial Intelligence model developed by the International Finance Corporation (a member of the World Bank Group). MALENA uses Natural Language Processing techniques to identify environmental, social and governance (or ESG) risks and analyze its significance. 

MALENA leverages long standing corpora of IFC ESG data and institutional knowledge to identify and assess ESG risks. By applying an in-house adapted Sentiment Analysis methodology, MALENA provides easy-to-access project and portfolio insights. MALENA complements investment project ESG due diligence by our Specialists and supports better-informed decision making. 

IFC’s Malena project has the potential to revolutionize ESG investment in emerging markets by using a data-driven approach to attract investors to invest in emerging market capital markets. By accessing the MALENA AI emerging market investors can screen target companies using IFC’s ESG Standards as the risk management framework powered by AI. This is expected to increase the amount of investment in emerging market capital markets and, in doing so, advance ESG integration globally to achieve environmental, social and climate friendly outcomes.

The MALENA AI model was developed using supervised machine learning, an effective but time and data intensive approach that demands a lot of interaction between the “supervisors” (data analysts and data scientists) and the algorithm. Supervised machine learning requires the creation of large sets of labeled data to train the model to classify data and replicate outcomes on new datasets. These labeled datasets prepared by a team of ESG analysts support data scientists who train and finetune the NLP model. MALENA sentiment analysis required the development of a methodology that must be consistently applied to training data to then train the NLP model. We will discuss the methodology, rules and metrics used for creating a training dataset of 150,000+ ESG analyst-annotated data points for tense-based sentiment analysis and severity assessment which is derived from historical sustainability corpus data and expertise acquired by IFC experience of 20+ years conducting ESG due diligence for its investing projects.
",,,
600,4,sem23-industry,Cultivating IoT Success—How Growing Bonsais Revealed the Future of Industrial IoT,"Thomas Vout, Peter Crocker and Mikkel H. Brynildsen",2023-06-13 17:36,1970-01-01 00:00,(Demo link) ,"IoT
Internet of Things
Knowledge Graph
Semantic Reasoning
Rules-Based AI
Edge Device
Raspberry Pi
Mobile Device
Automation
Sensor Network
Predictive Maintenance
Component Management
Network Management
Logistics Management
Enterprise Optimization
Industrial Efficiency
Gardening
Horticulture",accept,"Today, the Internet of Things (IoT) touches every corner of the globe. This demo illustrates how knowledge graphs and semantic reasoning are revolutionizing practices such as predictive maintenance and component management for companies worldwide—from the world’s largest buildings, logistics networks, offshore platforms and industrial plants, to one of nature’s smallest—the bonsai tree. 

Bonsai trees are tough to look after with relatively complex and specific needs, so maintaining a garden of 20 (including different species, each with their own preferences no less) is no small feat. Critical to their survival is the question: ‘When should I water?’—asked by their owners daily. Too much water suffocates the tree and rots the roots, too little and it’s not long before years of effort are lost. Even harder is the question of what to do when you’re away from home. 

The solution lies in soil sensors that monitor moisture levels for a selection of the trees; reporting their readings every minute. Along with weather data, these readings are processed on a central hub—a Raspberry Pi running the knowledge graph and semantic reasoning engine RDFox. Based on this data, the hub can issue alerts to the owner and control a series of pumps, each of which irrigates a different group of trees. The network of sensors, devices, and pumps, along with the trees themselves, are represented as a graph in RDFox, capturing functional properties and their relationships. By incorporating real-time sensor data and applying semantic reasoning, the system intelligently determines when individual trees need watering, activating the appropriate pump and giving the thirsty plants the drink they need. 

Although clearly simplified in this scenario, the same principles can be scaled up to the vast and complex situations found in industry and nature. By using semantic reasoning in local hubs, IoT capabilities, industrial control and maintenance, and predictive analytics can be pushed out to the edge, bringing several benefits to real-world situations such as a complex of buildings, ships, or processing plants. By optimizing the device network, upfront capital can be reduced alongside ongoing operational costs. The same can be said for the system being monitored by improving the efficiency of the system itself and increasing the lifespan of its assets through reduced wear. 

With this demo, we’ll show how this can all be achieved with a knowledge graph. ",,,
640,1,sem23-research,Semantifying the PlanQK Platform and Ecosystem for Quantum Applications,"Darya Martyniuk, Naouel Karam, Michael Falkenthal, Yufan Dong and Adrian Paschke",2023-05-23 15:18,2023-05-30 20:16,"(Paper Type) Short
(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) PlanQK Ontology: https://github.com/PlanQK/semantic-services.
(Ontology Availability) 
(Demo link) PlanQK platform: https://platform.planqk.de/.","Knowledge Graph
Ontology
Semantic Web
Semantic Search
Faceted Search
Ontology Based Search
Quantum Computing",accept,"Purpose: The goal of this paper is to report about an application of a knowledge semantification approach for the “platform and ecosystem for quantum applications” (PlanQK). Specifically, we describe how the quantum computing knowledge, which is submitted on the platform by scientific community and industry actors, is incorporated into the platform knowledge graph (KG) and semantically annotated. Furthermore, we delve into the KG-based semantic search we developed for the platform.
Methodology: To achieve the data transformation from a relational database (RDB) into a machine-readable Resource Description Framework (RDF) format, we define a mapping between the database and ontology schemes, handle the data semantics and store the instance data in a unified KG. Moreover, we discuss a mechanism for a continuous synchronization of the knowledge in the graph. The semantic search we present further in this paper incorporates both native and faceted search techniques, leveraging the indexed parts of the KG.
Findings: The paper shows an on-the-fly solution that seamlessly integrates ontology and a web platform data stored in a relation database into a KG. It outlines the semantic search over the graph, which is not restricted to a single graph but can be also executed across an ecosystem of the connected graphs.
Value: RDB to RDF mapping and semantic search are complex tasks that require an architecture, which supports an effortless data integration and evolution over time. The presented solution is not limited to the PlanQK platform and quantum computing domain but can be applied in other applications as well. In further work, the PlanQK KG could be made available as Linked Data and connected with related graphs to build a quantum computing knowledge system.",https://doi.org/10.3233/SSW230017,10.3233/SSW230017,https://2023-eu.semantics.cc/img/illustrations/640-Semantic23-Drawing-Text-02.png
650,1,sem23-research,Knowledge-Grounded Target Group Language Recognition in Hate Speech,"Paula Reyero Lobo, Enrico Daga, Harith Alani and Miriam Fernandez",2023-05-09 11:35,2023-05-31 11:52,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/hate-speech-identities-B207/
(Data Availability) https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv; Jigsaw Toxicity Corpus
https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech; Measuring Hate Speech
https://osf.io/edua3/; Gab Hate Corpus
https://github.com/hate-alert/HateXplain/tree/master/Data; HateXplain
https://github.com/antmarakis/xtremespeech; Xtremespeech
(Ontology Availability) https://github.com/Superraptor/GSSO; Gender, Sex, and Sexual Orientation Ontology
https://github.com/dlab-projects/hate_target ; Roberta_base model
https://github.com/XuhuiZhou/Toxic_Debias/tree/main/data ; Toxic Debias
(Demo link) ","Hate Speech
Semantic Enrichment
Knowledge Graph
Language Models",accept,"Hate speech comes in different forms depending on the communities targeted, often based on factors like gender, sexuality, race, or religion. Detecting it online is challenging because existing systems are not accounting for the diversity of hate based on the identity of the target and may be biased towards certain groups, leading to inaccurate results. Current language models perform well in identifying target communities, but only provide a probability that a hate speech text contains references to a particular group. This lack of transparency is problematic because these models learn biases from data annotated by individuals who may not be familiar with the target group. To improve hate speech detection, particularly target group identification, we propose a new approach that incorporates explicit knowledge about the language used by specific identity groups. We leverage a Knowledge Graph (KG) and adapt it, considering an appropriate level of abstraction, to recognise hate speech-language related to gender and sexual orientation. A thorough quantitative and qualitative evaluation demonstrates that our approach is as effective as state-of-the-art language models while adjusting better to domain and data changes. By grounding the task in explicit knowledge, we can better contextualise the results generated by our proposed approach with the language of the groups most frequently impacted by these technologies. This helps us examine model outcomes and the training data used for hate speech detection systems, and handle ambiguous cases in human annotations more effectively. Overall, integrating semantic knowledge in hate speech detection is crucial for enhancing understanding of model behaviors and addressing biases derived from training data.",https://doi.org/10.3233/SSW230002,10.3233/SSW230002,https://2023-eu.semantics.cc/img/illustrations/650-Semantic23-Drawing-Text-03.png
671,4,sem23-industry,"Extracting ESG (Environment, Social, Governance) Knowledge from Global News using TextDistil",Prasad Yalamanchi,2023-06-12 22:03,1970-01-01 00:00,(Demo link) (demo will be available and turned on during the presentation),"Neural NLP
NLU
Semantic Technology
LLM
Ontology
Taxonomy
Reasoning
SPARQL
RDF
Knowledge Graph",accept,"Presentation covers the technology used to automate knowledge extraction from Text.  This novel technology blends large language models, semantic technology, rule systems, linguistic theory to achieve reliable performance. Specifically, the dicussion will focus on the integration with Dow Jones/Factiva news service, involving the extraction of facts buried in news articles, news letters and reports about the subject area of ESG (Environmental, Social and Governance) through the application of ESG taxonomy and ontology.  Extracted facts are output as RDF triples and ingested into a Semantic Knowledge Graph.  Knowledge Graph powers a verifiable and reliable search that is more a multi question answering interface.  Users get to directly interrogate the text corpus (news corpus) to get answers that are assembled from snippets of text from multiple underlying articles. ",,,
760,1,sem23-research,BiPaSs: Further Investigation of Fast Pathfinding in Wikidata,Leon Martin,2023-05-08 06:55,2023-07-06 07:33,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/uniba-mi/bipass-wikidata-pathfinder
(Data Availability) https://github.com/uniba-mi/bipass-wikidata-pathfinder/blob/main/data/wikidata_queries_10000_topics_genre.csv
(Ontology Availability) 
(Demo link) ","knowledge graphs
pathfinding
hyperparameter optimization
Wikidata",accept,"Purpose: A previous paper proposed a bidirectional A* search algorithm for quickly finding meaningful paths in Wikidata that leverages semantic distances between entities as part of the search heuristics. However, the work lacks an optimization of the algorithm’s hyperparameters and an evaluation on a large dataset among others. The purpose of the present paper is to address these open points.

Methodology: Approaches aimed at enhancing the accuracy of the semantic distances are discussed. Furthermore, different options for constructing a dataset of dual-entity queries for pathfinding in Wikidata are explored. 20% of the compiled dataset are utilized to fine-tune the algorithm’s hyperparameters using the Simple optimizer. The optimized configuration is subsequently evaluated against alternative configurations, including a baseline, using the remaining 80% of the dataset.

Findings: The additional consideration of entity descriptions increases the accuracy of the semantic distances. A dual-entity query dataset with 1,196 entity pairs is derived from the TREC 2007 Million Query Track dataset. The optimization yields the values 0.699/0.109/0.823 for the hyperparameters. This configuration achieves a higher coverage of the test set (79.2%) with few entity visits (24.7 on average) and moderate path lengths (4.4 on average). For reproducibility, the implementation called BiPaSs, the query dataset, and the benchmark results are provided.

Value: Web search engines reliably generate knowledge panels with summarizing information only in response to queries mentioning a single entity. This paper shows that quickly finding paths between unseen entities in Wikidata is feasible. Based on these paths, knowledge panels for dual-entity queries can be generated that provide an explanation of the mentioned entities’ relationship, potentially satisfying the users’ information need.",https://doi.org/10.3233/SSW230009,10.3233/SSW230009,https://2023-eu.semantics.cc/img/illustrations/760-Semantic23-Drawing-Text-04.png
905,2,sem23-pd,"Generate, Store, and Publish FAIR Data in Experimental Sciences","Nick Garabedian, Ilia Bagov, Malte Flachmann, Nuoyao Ye, Miłosz Meller, Floriane Bresser and Christian Greiner",2023-07-05 09:59,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) https://zenodo.org/record/8089376
(Data Availability) https://zenodo.org/record/8024881 ; Copper Tribology FAIR Data Experiments - Sapphire Counterbody
(Ontology Availability) https://zenodo.org/record/7923148 ; Vocabulary of Materials Tribology Lab at KIT
(Demo link) https://youtu.be/lS8w-LwwGU4","FAIR Data
R&D Data Management
Materials Science
Tribology",accept,"Purpose: FAIR data is a relatively new paradigm in research data management which aims to facilitate reproducibility of research, knowledge generation, and knowledge retention in all scientific domains. This paper presents a framework which enables the semantic generation, storage, and publications of FAIR datasets in the field of experimental materials science with the help of controlled vocabularies.

Methodology: The framework presented in this work consists of multiple software tools developed by the authors, as well as an external electronic lab notebook (ELN), which is used as a database. The centerpiece of this solution is VocPopuli, a tool for the collaborative development of FAIR SKOS-based controlled vocabularies. These vocabularies are used as the basis of further software components developed by the authors, which enable the entry, processing, and publishing of FAIR datasets.

Findings: This paper shows that SKOS-based controlled vocabularies can be used as the cornerstone of FAIR data management systems in experimental materials science, and research and development as a whole. Furthermore, it demonstrates how these vocabularies can be implemented in common laboratory workflows in a seamless fashion which simplifies the generation, storage, and publication of FAIR data.

Value: The solution presented in this work enables the simplified creation of FAIR data with minimal user input and intervention. The controlled vocabularies, which are used to define the schemas of the generated datasets, facilitate the linking of external semantic resources, and increase the reproducibility of the research results. Furthermore, using our framework these datasets can easily be published to open science platforms, so that other researchers can benefit from their insights.",https://ceur-ws.org/Vol-3526/paper-01.pdf,,
1161,2,sem23-pd,OntoAnon: An Anonymizer For Sharing Ontology Structure Without Data,"Achim Reiz, Robert Schlücker and Kurt Sandkuhl",2023-07-05 06:52,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) https://github.com/Uni-Rostock-Win/OntoAnon
(Data Availability) https://github.com/Uni-Rostock-Win/ontoAnon-Testdata; Testdata
(Ontology Availability) 
(Demo link) https://github.com/Uni-Rostock-Win/OntoAnon/releases","Ontology
Anonymizer
OOPS
NEOntometrics
OWL
RDF",accept,"Getting insights into the ontology developments of organizations is as valuable as it is scarce. On the one hand, these insights provide information into the specifics of corporate ontology developments, like the used language constructs or different evolution processes. On the other hand, these ontologies often store sensitive information on products, processes, and the overall business, and enterprises are (understandably) reluctant to share them with persons outside their organization.
To ease the collaboration between academia and practice, we present ""OntoAnon"", an anonymizer for ontologies. The Python-based application allows the removal of sensitive information like class-, and property names, as well as annotation contents, while preserving the ontology structure and used formalisms.
",https://ceur-ws.org/Vol-3526/paper-06.pdf,,
1331,1,sem23-research,Methodology for Creating a Knowledge Graph with RDF Reification,"Manoé Kieffer, Ginwa Fakih and Patricia Serrano Alvarado",2023-05-23 17:56,2023-05-31 08:41,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/pipeline
https://anonymous.4open.science/r/queries_comparisons
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Knowledge graph
RDF reification
Query evaluation
Experimental comparison
Educational resources",accept,"Purpose: This paper presents the construction of a Knowledge Graph (KG) of Educational Resources (ER), where RDF reification is essential. The ERs are described based on the subjects they cover considering their relevance. RDF reification is used to incorporate this subject's relevance. Multiple reification models with distinct syntax and performance implications for storage and query processing exist. This study aims to experimentally compare four statement-based reification models on four triplestores to determine the most pertinent choice for our KG. 

Methodology: The authors built four versions of the KG. Each version has a distinct reification model, namely standard reification, singleton properties, named graphs, and RDF-star, which were obtained using RML mappings. The KG consists of 45,000 ERs, 13,000 authors, 135,000 subjects, and 8,250,000 statements linking the ERs to their subjects. Each of the four triplestores (Virtuoso, Jena, Oxigraph, and GraphDB) was set up four times (except for Virtuoso, which does not support RDF-star), and seven different SPARQL queries were experimentally evaluated. 

Findings: In general, this study shows that standard reification and named graphs lead to good performance. It also shows that Virtuoso outperforms Jena, GraphDB, and Oxigraph in most queries, in the particular context of the used KG.

Value: The recent specification of RDF-star and SPARQL-star sheds light on statement-level annotations. The empirical study reported in this paper contributes to the efforts toward the efficient usage of RDF reification. In addition, this paper shares the pipeline of the KG construction using standard semantic web technologies.",https://doi.org/10.3233/SSW230008,10.3233/SSW230008,https://2023-eu.semantics.cc/img/illustrations/1331-Semantic23-Drawing-Text-05.png
1369,1,sem23-research,Perplexed by idioms?,"J. Nathanael Philipp, Max Kölbl, Erik Daas, Yuki Kyogoku and Michael Richter",2023-05-23 07:48,1970-01-01 00:00,"(Paper Type) Short
(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) eng_news_2020_1M       https://corpora.uni-leipzig.de
eng_wikipedia_2016_1M  https://corpora.uni-leipzig.de
BNC_idioms             https://github.com/bondfeld/BNC_idioms
(Ontology Availability) 
(Demo link) ","perplexity
idiomatic expressions
literal expressions
information theory",accept,"Purpose: In an information theoretic framework, this study aims to identify idiomatic expressions (IE) in English by measuring perplexity between texts with IE, without IE (literals) and large reference texts. 

Methodology: Four texts are analysed: one text with at least one IE per sentence, one text without any IE, and two large, sentence-wise unrelated reference texts . Perplexity is measured based on bi- till heptagrams of PoS tags and tokens, and , in addition, of thematic roles within the boundaries of a sentence extracted by the LOME parser. We assume that the divergence of perplexity from the reference would be higher for IEs compared to literals. 

Findings: We observed that there is no significant difference in perplexity between IE, literal expressions and reference texts.

Value: Model-based identification of IE can be useful in automatic speech processing applications, which sometimes have problems with IE. We suggest utilising larger extra-sentential contexts to determine perplexity. Additionally, reducing the number of thematic roles can avoid an uniform distribution of n-grams.",https://doi.org/10.3233/SSW230006,10.3233/SSW230006,https://2023-eu.semantics.cc/img/illustrations/1369-Semantic23-Drawing-Text-06.png
1683,1,sem23-research,Cobalt: A Content-Based Similarity Approach for Link Discovery over Geospatial Knowledge Graphs,"Alexander Becker, Abdullah Ahmed, Mohamed Ahmed Sherif and Axel-Cyrille Ngonga Ngomo",2023-05-04 16:27,2023-05-25 13:06,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/LIMES-A738
(Data Availability) https://ec.europa.eu/eurostat/de/web/gisco/geodata/reference-data/administrative-units-statistical-units/NUTS ; NUTS
https://land.copernicus.eu/pan-european/corine-land-cover ; CLC
(Ontology Availability) 
(Demo link) ","Linked Data
Knowledge graphs
Geospatial Knowledge graphs
Matching
KG Integration
Topological Relations",accept,"Purpose: Data integration and applications across knowledge graphs (KGs) rely heavily on the discovery of links between resources within these KGs. Geospatial link discovery algorithms have to deal with millions of point sets containing billions of points.
Methodology: To speed up the discovery of geospatial links, we propose Cobalt. Cobalt combines the content measures with R-Tree indexing. The content measures work based on the area, diagonal and distance of polygons which speeds up the process but is not perfectly accurate. We thus propose a polygon splitting approach for improving the accuracy of Cobalt.
Findings: Our experiments on real-world datasets show that Cobalt is able to speed up the topological relation discovery over geospatial KGs by up to 1.47*10^4 times over other linking algorithms while maintaining an F-Measure between 70% and 90% depending on the relation. Furthermore, we were able to achieve an F-Measure of up to 99% by splitting the polygons before applying the content measures.
Value: The process of discovering links between geospatial resources can be significantly faster by sacrificing the optimality of the results. This is especially important for real-time data-driven applications such as emergency response, location-based services and real-time traffic management. In the future additional measures, like the location of polygons or the name of the entity a polygon represents, could be integrated to further improve the accuracy of the results.",https://doi.org/10.3233/SSW230013,10.3233/SSW230013,https://2023-eu.semantics.cc/img/illustrations/1683-Semantic23-Drawing-Text-07.png
1796,4,sem23-industry,Building the Dutch Linked Data Space for Cultural Heritage,Enno Meijers,2023-05-09 05:53,2023-05-09 05:55,"(Demo link) Some of the available components developed in our program: 
https://datasetregister.netwerkdigitaalerfgoed.nl/?lang=en
https://termennetwerk.netwerkdigitaalerfgoed.nl/
All software is developed as open source under the EUPL license, see details on our Github: https://github.com/netwerk-digitaal-erfgoed","Cultural Heritage
Decentralization
Data Space
Federated querying
Linking
Knowledge Graphs
Linked Data services",accept,"In order to improve the access to cultural heritage information the Dutch cultural heritage institutes decided in 2015 to intensify their cooperation and create the Dutch Digital Heritage Network (NDE). Supported by the ministry of Culture, NDE runs a long term program aimed at increasing the social value of the cultural heritage information available through libraries, archives, museums and other cultural institutions, by making their collections more visible, usable and sustainable.

The core of the NDE-program is about improving interoperability by implementing Linked Data standards and technologies throughout the network. An increasing number of cultural heritage institutions are publishing their data as Linked Data and standardizing their data by adding links to shared terminology sources. The NDE program has worked closely with vendors to develop the supporting functionality to do so. A nationwide, federated Network-of-Terms service was built to support easy access to shared terminology. A self updating Dataset Registry is operational containing 500+ Linked Datasets. Knowledge Graph services are being developed to improve findability of relevant datasets.

Currently the focus of the NDE program is on the design of platforms for end user services that demonstrate the added value of interlinked collections. Together with vendors of web platforms we are exploring the possibilities to create new, dynamic end user services such as linked data stories, browsers and timelines.  

In our contribution we will present our long term approach on the implementation of Linked Data in the Dutch cultural heritage domain and show examples of results we have made so far.",,,
1798,2,sem23-pd,SPARQLGEN: One-Shot Prompt-based Approach for SPARQL Query Generation,"Liubov Kovriguina, Roman Teucher, Daniil Radyush and Dmitry Mouromtsev",2023-07-05 09:09,1970-01-01 00:00,"(Publication Resources) N/A
(Code Availability) 
(Data Availability) https://github.com/ag-sc/QALD/tree/master/9/data ; QALD-9 
https://github.com/KGQA/QALD-10 ; QALD-10 
https://anonymous.4open.science/r/sparqlgen-102F/beastiary_with_qald_format.json ; BESTIARY
(Ontology Availability) https://platform.openai.com/docs/models/gpt-3-5 ; GPT-3.5 model for query generation

https://anonymous.4open.science/r/sparqlgen-102F/README.md ; Supplementary material, that includes augmented test data for QALD9, QALD10 and BESTIARY datasets, error distribution and detailed error analysis of generated queries for QALD10, as well as extra images.
(Demo link) The paper is submitted as poster.","Knowledge Graphs Question Answering
SPARQL query generation
Augmented Large Language Models
Prompt Template Design",accept,"In this work, we present a one-shot generative approach (further referred to as SPARQLGEN) for generating SPARQL queries by augmenting Large Language Models (LLMs) with the relevant context within a single prompt. The prompt includes heterogeneous data sources: the question itself, an RDF sub-graph required to answer the question, and an example of a correct SPARQL query for a different question. In the experiments, GPT-3, a popular pre-trained language model from OpenAI, was leveraged, but it is possible to extend the approaches to any other generative LLM. We evaluate, how  different types of context in the prompt influence the query generation performance on QALD-9, QALD-10 and Bestiary dataset (BESTIARY), which was created to test LLM performance on unseen data, and provide a detailed error analysis. One of the findings is that providing the model with the underlying KG and a random correct query improve the generation results. The approach shows strong results on QALD-9 dataset, but doesn't generalize on QALD-10 and BESTIARY, which can be caused by memorization problem. ",https://ceur-ws.org/Vol-3526/paper-08.pdf,,
1892,4,sem23-industry,A Nucleus of a European Legal Data Space,Christian Dirschl and Anke Losch,2023-05-30 10:00,2023-06-22 09:45,(Demo link) ,"Legal Domain
European Data Space
DataBri-X
Data Governance",accept,"We will present a nucleus of a European Legal Data Space, so a Data Space that aims to cover legal information both for business and research.
Currently there does not exist a European Legal Data Space. This means that people working with legal data have no platform to execute proper NLP or AI. Existing models etc. do usually not work, since the legal language is very specific. Also the legal situation is different in the European countries and therefore also language specific.
However legal startups, public administrations and governments are more and more depending on tools and models that are specifically tailored to legal data. We want to make a first contribution to this growing need with making a platform available, making initial content sets available and an integrated toolset.
We will go through the general architecture, the initial use cases and specific requirements and limitations that come from the domain.
We will show how this Data Space fits into the general objectives of the EU funded Databri-X project, which is focusing on Data Governance.
We will make a short demo and talk about the next steps for enhancing the prototype.",,,
1999,4,sem23-industry,Sustaining ESG intelligence: Large Language Models and Knowledge Graphs,Madi Solomon and James Phare,2023-07-06 08:16,1970-01-01 00:00,(Demo link) ,"knowledge graphs
LLM
ontology
taxonomy
ESG",accept,"In the sustainable finance sector, international financial regulators & standards bodies have created over 30 extensive green taxonomies that provide guidance on how to monitor climate change and emerging regulations.

In order to optimise these large industrial classification schemes, Neural Alpha began by creating a knowledge graph powered by natural language processing (NLP). By using ontologies to connect concepts, they developed a contextually rich, high performance data product that aids in faster and easier sustainable financial decision making.

In this fascinating session, James Phare of Neural Alpha and Madi Solomon of Graphifi share recent developments of how pairing Large Language Modules with their Responsible Capital ESG Knowledge Graph supercharged the disclosure analysis and integration of data by augmenting human intelligence with conversational AI.

",,,
2360,2,sem23-pd,Scalable and No-Code Knowledge Graph Exploration and Querying with SemSpect,"Thorsten Liebig, Michael Opitz, Vincent Vialard and Maximilian Wenzel",2023-07-04 21:19,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) https://www.semspect.de/","Knowledge Graphs
RDF
LPG
Graph Visualization
Graph Exploration",accept,"How can we gain meaningful insight into large RDF graphs? Which groups of nodes are connected, what relationships does a single node have? This demo is addressing these challenges and will demonstrate SemSpect for RDF, a new tool which uses visual aggregation to solve the hairball problem of graph visualization. The aggregation approach enables users to explore large knowledge graphs, from the meta level down to all node details without any prior idea of the data. We will demonstrate how to build sophisticated queries in a data-driven manner without any query language skills.",https://ceur-ws.org/Vol-3526/paper-07.pdf,,
2647,2,sem23-pd,Enhancing Interpretability of Machine Learning Models over Knowledge Graphs,"Yashrajsinh Chudasama, Disha Purohit, Philipp D. Rohde and Maria-Esther Vidal",2023-07-05 07:56,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) https://github.com/SDM-TIB/InterpretME_Demo
https://github.com/SDM-TIB/InterpretME
(Data Availability) https://zenodo.org/record/8112628
(Ontology Availability) http://ontology.tib.eu/InterpretME/visualization ; InterpretME ontology
https://www.youtube.com/watch?v=Bu4lROnY4xg ; InterpretME demo video
https://pypi.org/project/InterpretME/ ; InterpretME python library
(Demo link) https://mybinder.org/v2/gh/SDM-TIB/InterpretME_Demo/main?labpath=InterpretME_Demo.ipynb","Interpretability
ML Models
Knowledge Graphs",accept,"Artificial Intelligence (AI) plays a critical role in data-driven decision-making frameworks. However, the lack of transparency in some machine learning (ML) models hampers their trustworthiness, especially in domains like healthcare. This demonstration aims to showcase the potential of Semantic Web technologies in enhancing the interpretability of AI. By incorporating an interpretability layer, ML models can become more reliable, providing decision-makers with deeper insights into the model’s decision-making process. InterpretME effectively documents the execution of an ML pipeline using factual statements within the InterpretME knowledge graph (KG). Consequently, crucial metadata such as hyperparameters, decision trees, and local ML interpretations are presented in both human- and machine-readable formats, facilitating symbolic reasoning on a model’s outcomes. Following the Linked Data principles, InterpretME establishes connections between entities in the InterpretME KG and their counterparts in existing KGs, thus, enhancing contextual information of the InterpretME KG entities. A video demonstrating InterpretME is available online, and a Jupyter notebook for a live demo is published in GitHub.",https://ceur-ws.org/Vol-3526/paper-05.pdf,,
3819,4,sem23-industry,We created a Knowledge Graph and want to show you how we did it,Rik Opgenoort and Redmer Kronemeijer,2023-05-09 07:29,1970-01-01 00:00,(Demo link) ,"etl
knowledge graph
architecture
ontology
enterprise content model
software",accept,"In this case study we present how we built our Knowledge Graph at CROW, an independent not-for-profit knowledge platform. We will show the architecture, name all the components, show which software is used for what, etc. We feel that this is a question that lingers with many visitors of SEMANTiCS: I want to build a knowledge graph, but how? We give an overview of the architecture and in this interactive session, zoom in on specific parts with questions from the audience.

Our semantic approach involved setting up a complete platform, including ETL’s (with Comunica, RML, etc.), triple stores (GraphDB, Laces Hub, TriplyDB, etc.), ontology managers (Vocbench, Laces Library Manager, etc.) and creating a unified ontology that defined the concepts and relationships within the organization's data. The result enabled employees to discover and analyse data in a more intuitive and efficient manner, and in general created awareness for the data-centric approach. As a spin-off the ontology became the start of our ‘Enterprise content model’.

CROW has accumulated a vast amount of knowledge over the years, but the data which hold this knowledge was siloed and semi-structured, making it difficult for clients and employees to access and utilize it effectively. As a result, there was a lack of consistency in decision-making, inefficient workflows, and missed opportunities for innovation.
 
As the organization continues to add to the Knowledge Graph, the value of the solution will increase. Moreover, as CROW expands its business, the Knowledge Graph will enable the organization to better manage its data and foresee in the client needs that are yet to come.
 
",,,
4210,4,sem23-industry,Towards a large-scale federated knowledge graph for government information,Jan Voskuil,2023-06-13 18:47,1970-01-01 00:00,(Demo link) ,"Knowledge graph
Data federation
Knowledge representation
RDF
Semantic Web
Linked Data
Geospatial data",accept,"Dutch government organizations have been actively involved in creating and publishing RDF knowledge graphs since 2012. Some initiatives have proven quite successful through the years. Currently, many RDF datasets are available, including RDF datasets containing information registered in so-called base registrations, which by law are assigned to provide an authoritative description of certain aspects of the world. Well-known cases are datasets from the bureau of land registry (Kadaster), but there are many more.

Most of these RDF knowledge graphs have been developed in a bottom-up fashion. Without much overall guidance, the datasets are predominantly expressed in terms of their own locally designed ontologies. Over the years, there have been initiatives to design shared ontologies as standards to be used in certain domains. When considering individual cases, results seem to be mixed. The overall trend, however, is that significant efforts are made to connect existing datasets, creating a large-scale federated knowledge graph for government information.

This presentation will discuss this general trend, focusing on a specific case. Currently, there is a dataset containing geometries describing administrative areas. Another dataset identifies government organizations, their properties and their history. The two datasets are maintained by different organizations, ultimately residing under the Ministry of the Interior and Kingdom Relations. The case illustrates what the impact of such an undertaking is on the locally designed ontologies. We discuss some recurrent modelling mistakes and how to deal with them. It is shown how a formal ontological based on Guizzardi’s work on UFO  and OntoUML supports effective ontology alignment.

Disclaimer: the author is involved in the work reported in this presentation. This work is commissioned by KOOP, the publications office of the Dutch government. All content is presented in the author's personal capacity. KOOP does not endorse any parts of the presentation.
",,,
4566,4,sem23-industry,Leveraging Semantic Technologies for Enhanced Enterprise Knowledge & Governance: A Real-World Use Case,Adrian Gschwend and Michael Rauch,2023-06-08 12:22,1970-01-01 00:00,(Demo link) ,"Enterprise Knowledge
Enterprise Governance
Knowledge Graphs
Data Integration
Enterprise Architecture
Data Cataloging
Compliance Systems",accept,"## Initial Situation

Large organizations often face a critical challenge: connecting their  enterprise architecture, data catalog, data lineage, and compliance systems. These  systems, crucial for smooth business operations, are often disjoint,  operating in isolation or 'silos'. This separation hampers not just data accessibility, but also broader aspects such as compliance tracking and architectural coherence. Our financial services client confronted this  precise issue, highlighting the urgent need for a solution that could  seamlessly integrate these key areas.

## Approach and IT-Solution

Our solution's distinctive edge lies in its capacity to unify enterprise architecture, data cataloging, and compliance systems using the RDF graph model. By transforming diverse data into RDF triples, and employing ontologies, rules, and reasoning, we transform disjointed data into an interconnected information landscape.

In addition to data integration, we have developed a user-friendly frontend that offers faceted search and interactive exploration of the graph model. This interface enhances user engagement with the system, encouraging data discovery and insights generation.

Our deployment process, underpinned by CI/CD principles, handles various data formats, enhancing data pipeline management. Our frontend will be released as open-source software, enabling other organizations to benefit from our approach.

## Success Criteria for / Benefit of the Semantic Solution

Our solution's success is best demonstrated by the substantial benefits experienced by our financial industry customer. By integrating metadata from diverse sources, the system allowed them to feed metadata daily from over 400 databases containing more than 42,000 tables into a comprehensive knowledge graph. Additionally, we were able to unify disparate but crucial organizational components like the agile service delivery organization (including arena, feature team, release train), employee data, domain architecture information, and business processes.

This approach has enhanced visibility and accessibility of information for the 60+ recurring users of our system. With over 17+ million triples contained, users can now traverse data from all sources in milliseconds, illustrating the system's enhanced performance and scalability.

## Prospects and Recommendations

Semantic technologies offer great potential to unify enterprise architecture, data catalogs, and compliance systems. For businesses planning to use such technologies, we recommend starting with the most problematic data silos, gradually expanding the integration.

We've seen that complex organizations can reduce complexity by adopting a data-centric approach with ontologies as a schema layer.  Our experience suggests that even complex enterprises might need only a few hundred concepts as their core model, leading to a complexity reduction of 10-100 times.

As future steps, we aim to handle more data and connect more organization components, making semantic technologies benefits more widespread.

## Demo

In our presentation, we will provide a short demonstration of our system, showcasing the real-world application of semantic technologies and the transformative effect they have on an organization's enterprise knowledge and governance.",,,
4716,4,sem23-industry,Fusing Semantics and Property Graphs to power Enterprise Recommender Systems,"Robert David, Astrid Krickl and Jesús Barrasa",2023-06-05 09:55,1970-01-01 00:00,(Demo link) ,"Neo4j
Graph database
Property graph
Analytics
Recommender",accept,"Knowledge Graphs have become increasingly important for knowledge representation and data consolidation in enterprises. They provide the basis to solve complex data problems, provide insights into data and thereby support intelligent decision making at the digital workplace. 

Knowledge Graphs technologically developed  into two distinct directions. The Semantic Web stack provides standardised technologies for semantic descriptions, interoperability and reasoning, whereas the property graph domain is focused on high-performance problem solving based on graph structures.

In this industry talk, we present an approach to bring together these two domains to build a knowledge-based enterprise recommender that is powered by a semantic knowledge graph and that processes the recommendations using Neo4j and a property graph representation.

We present the use case of analysing ESG-related documents and supporting the writing of ESG reports by providing intelligent insights into ESG standard documents.

-Initial Situation
SWC is developing innovative enterprise recommender solutions based on Semantic AI. We want to leverage the modelling capabilities and scaling power of the Neo4j property graph database to support this scenario by integrating it with PoolParty Semantic Suite.

- Approach and IT-Solution
We develop an integrated architecture that can bridge the Semantic Web and property graph domain and leverage the best of both worlds. We use neosemantics to connect these technologies, which fits RDF data into the property graph before rebuilding the data to benefit from the properties of the relations. We utilize annotations of ESG-relevant documents generated by the PoolParty Extractor as data. Using these techniques, we can exploit semantically enriched data in the scalable Neo4j context.

-Success Criteria for / Benefit of the Semantic Solution
We demonstrate the solution based on an ESG use case and data set and show how the recommender provides valuable insights into ESG topics. To successfully build different paths for recommendations we need to restructure the RDF data in Neo4j knowingly. Properties that are relevant for the recommendation results have to be stored appropriately to use the advantages of property graphs fully.

- Prospects and Recommendations
As a next step, the developed prototype will be brought into a corporate setting to gain experience with different use cases and further develop the quality of enterprise recommendations.

- Demo (if applicable)
We will show a life demo of the prototype based on an ESG use case.",,,
4723,1,sem23-research,Native Execution of GraphQL Queries over RDF Graphs Using Multi-way Joins,"Nikolaos Karalis, Alexander Bigerl and Axel-Cyrille Ngonga Ngomo",2023-05-09 07:15,2023-05-31 10:34,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) https://drive.google.com/file/d/1sJ3-rcHeSncvOJqSeDKn4K_7hpuiXRUu
(Demo link) ","graphql
knowledge graphs
multi-way joins",accept,"Purpose: The query language GraphQL has gained significant traction in recent years.
In particular, it has recently gained the attention of the semantic web and graph database communities and is now often used as a means to query knowledge graphs.
Most of the storage solutions that support GraphQL rely on a translation layer to map the said language to another query language that they support natively, for example SPARQL.
Methodology: Our main innovation is a multi-way left-join algorithm inspired by worst-case optimal multi-way join algorithms. This novel algorithms enables the native execution of GraphQL queries over RDF knowledge graphs. 
We evaluate our approach in two settings using the LinGBM benchmark generator.
Findings: The experimental results suggest that our solution outperforms the state-of-the-art graph storage solution for GraphQL with respect to both query runtimes and scalability. 
Value: Our solution is implemented in  an open-sourced triple store, and is intended to advance the development of representation-agnostic storage solutions for knowledge graphs.",https://doi.org/10.3233/SSW230007,10.3233/SSW230007,https://2023-eu.semantics.cc/img/illustrations/4723-Semantic23-Drawing-Text-08.png
4875,1,sem23-research,Semantics for Implementing Data Reuse and Altruism under EU's Data Governance Act,"Beatriz Esteves, Víctor Rodríguez Doncel, Harshvardhan J. Pandit and David Lewis",2023-05-09 22:28,2023-05-31 09:52,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/dgaterms-F0C8/
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Data Governance Act
Semantic Web
Machine-readable policies
Data intermediaries
Data altruism
Registers of Activities",accept,"Purpose: Following the impact of the GDPR on the regulation of the use of personal data of European citizens, the European Commission is now focused on implementing a common data strategy to promote the (re)use and sharing of data between citizens, companies and governments while maintaining it under the control of the entities that generated it. In this context, the Data Governance Act (DGA) emphasizes the altruistic reuse of data and the emergence of data intermediaries as trusted entities that do not have an interest in analysing the data itself and act only as enablers of the sharing of data between data holders and data users.
Methodology: In order to address DGA's new requirements, this work investigates how to apply existing Semantic Web vocabularies to (1) generate machine-readable policies for the reuse of public data, (2) specify data altruism consent terms and (3) create uniform registers of data altruism organisations and intermediation services' providers.
Findings: In addition to promoting machine-readability and interoperability, the use of the identified semantic vocabularies eases the modeling of data-sharing policies and consent forms across different use cases and provides a common semantic model to keep a public register of data intermediaries and altruism organisations, as well as records of their activities. Since these vocabularies are openly accessible and easily extendable, the modeling of new terms that cater to DGA-specific requirements is also facilitated.
Value: The main results are an ad-hoc vocabulary with the new terms and examples of usage, which are available at https://anonymous.4open.science/r/dgaterms-F0C8/. In future research, this work can be used to automate the generation of documentation for the new DGA data-sharing entities and be extended to deal with requirements from future data-related regulations.",https://doi.org/10.3233/SSW230015,10.3233/SSW230015,https://2023-eu.semantics.cc/img/illustrations/4875-Semantic23-DrawingText-09.png
4985,4,sem23-industry,Knowledge Models as Silver Bullet for Quality Intelligence,Dr. Martin Ley and Johann Wagner,2023-05-31 07:55,1970-01-01 00:00,(Demo link) ,"Pharmaceutical Industry
Thesaurus
Controlled Vocabulary
CDISC SEND
Versioning with PROV-O
Thesaurus & Ontology Management
Knowledge Graph
Information Retrieval
Information Delivery",accept,"In many highly regulated industries, in particular the pharmaceutical industry, controlled vocabularies are state of the art. In addition, companies typically provide their own terminology. Internal as well as standard vocabularies may refer to different concepts and are subject to changes over time. 

This situation very often makes it a herculean task to compare labels and/or concepts, update vocabularies, trace changes, and, after all, make use of controlled vocabularies for downstream processes such as information retrieval and information delivery. For instance, a content management system may contain tags from previous versions of the vocabulary, and users need to de-reference those labels or trace their life cycle.

In this talk, we illustrate how we solved these challenges for an industry partner in the pharmaceutical industry with its high standards for data quality and system/process validation. We show how we combined public and custom-made ontologies, integrated vocabularies from various sources and integrated mechanisms for versioning based on PROV-O in our knowledge model. In addition, we demonstrate how a querying module is integrated into the customer’s workflows, providing the basis for future applications such as autoclassification, information retrieval and intelligent information delivery. 
",,,
5020,1,sem23-research,Classification of Linking Problem Types for linking semantic data,"Raphaël Conde Salazar, Clement Jonquet and Danai Symeonidou",2023-05-10 08:06,2023-05-30 21:20,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Data linking
Semantic web
Linking Problem Types
Classification",accept,"As the number of RDF datasets published on the semantic web continues to grow, it becomes increasingly important to efficiently link similar entities between these datasets. However, the performance of existing data linking tools, often developed for general purposes, seems to have reached a plateau, suggesting the need for more  modular and efficient solutions. In this paper, we propose --and formalize in OWL-- a classification of the different Linking Problem Types (LPTs) to help the linked data  community identify upstream the problems and develop more efficient solutions. Our classification is based on the description of heterogeneity reported in the  literature --especially five articles-- and identifies five main types of linking problems: predicate value problems, predicate problems, class problems, subgraph problems, and graph problems. By classifying LPTs, we provide a framework for understanding and addressing the challenges associated with semantic data linking. It can be used to develop new solutions based on existing modularized tools addressing specific LPTs, thus improving the overall efficiency of data linking.",https://doi.org/10.3233/SSW230014,10.3233/SSW230014,https://2023-eu.semantics.cc/img/illustrations/5020-Semantic23-DrawingText-10.png
5195,1,sem23-research,Using Pre-trained Language Models for Abstractive DBpedia Summarization: A Comparative Study,"Hamada Zahera, Fedor Vitiugin, Mohamed Sherif, Carlos Castillo and Axel-Cyrille Ngonga",2023-05-08 04:00,2023-07-10 11:46,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/dice-group/DBpedia-Summarizer
(Data Availability) https://zenodo.org/record/7600894
(Ontology Availability) 
(Demo link) ","DBpedia Abstracts
Abstractive Summarization
Large Language Model
Knowledge Graph",accept,"Purpose: 
This study addresses the limitations of current short abstracts of DBpedia entities, which often lack a comprehensive overview due to their creating method (i.e., selecting the first two-three sentences from the full DBpedia abstracts). 
We propose an abstractive summarization approach using pre-trained language models, which can generate concise and informative summaries.

Methodology: 
We leverage pre-trained language models to generate abstractive summaries of DBpedia abstracts in six languages (English, French, German, Italian, Spanish, and Dutch). 
We performed several experiments to assess the quality of generated summaries by language models. In particular, we evaluated the generated summaries using human judgments and automated metrics (Self-ROUGE and BERTScore). 
Additionally, we studied the correlation between human judgments and automated metrics in evaluating the generated summaries under different aspects: informativeness, coherence, conciseness, and fluency.

Findings: 
Pre-trained language models generate summaries more concise and informative than existing short abstracts. Specifically, BART-based models effectively overcome the limitations of DBpedia short abstracts, especially for longer ones. Moreover, we show that BERTScore and ROUGE-1 are reliable metrics for assessing the informativeness and coherence of the generated summaries with respect to the full DBpedia abstracts. We also find a negative correlation between conciseness and human ratings. Furthermore, fluency evaluation remains challenging without human judgment.

Value: This study has significant implications for various applications in machine learning and natural language processing that rely on DBpedia resources. 
By providing succinct and comprehensive summaries, our approach enhances the quality of DBpedia abstracts and contributes to the semantic web community.",https://doi.org/10.3233/SSW230003,10.3233/SSW230003,https://2023-eu.semantics.cc/img/illustrations/5195-Semantic23-DrawingText-11.png
5522,4,sem23-industry,A New Approach to Taxonomy Creation: Combining Human Expertise with NLP to Extract New Concepts from Regulations and Law Articles,Kike Franssen and Eveline Schmidt,2023-05-30 09:19,1970-01-01 00:00,(Demo link) ,"Natural Language Processing
Generation of Taxonomy Concepts
Large Language Models
Ontology Generation
Named Entity Recognition
Entity Extraction
Machine Learning Techniques
Semantic Context in Data Science",accept,"Abstract:
To mitigate the risk of subjectivity and time-loss in the process of extracting new taxonomy concepts from regulations and law articles, we present a solution used by the Dutch National Police. The application uses entity recognition to mark existing concepts and generates a subset of new possible concepts. To generate this, Large Language Models are enriched with police-specific knowledge from taxonomies defined and curated by our modellers. These suggested concepts are then verified by these same modellers. This human touch, in combination with the Natural Language Processing tooling, increases the accuracy, efficiency and objectivity of the extraction process. 


Description:
The Dutch National Police faces the challenge of defining a vast array of concepts for their taxonomies and ontologies, aiming to create a unified semantic model that can be effectively utilized across the entire police organization. These concepts need to be manually extracted from regulations and law articles. Given the extensive nature of these documents and the subjective nature of the process, the manual selection of the concepts can introduce subjectivity and takes a substantial amount of time.

To counteract these problems, we present a solution using entity extraction and supervised Machine Learning to further populate the taxonomies by learning from the existing concepts. The solution enhances accuracy, efficiency, and objectivity, minimizing the potential for human error while keeping the human touch. 

With this solution, the Dutch National Police, in partnership with Ordina, uses Natural Language Processing to thoroughly analyse the textual documents. It marks the concepts already present in our police taxonomies and suggests new possible concepts. These concepts are selected by building on existing Large Language Models (LLM) and enriching them with police-specific taxonomies. The suggestions are generated by considering a variety of factors, including the semantic context surrounding police-specific concepts. This is achieved through the implementation of pre-processing techniques such as tokenization and lemmatization, as well as leveraging the power of transfer learning. As a result, the model can be trained effectively to produce meaningful results, even with a smaller dataset. The reason for this being that the LLM provides the base, while the taxonomies add another, more subject-specific layer. 

The outcome of using this tooling is two-fold:
- An annotated document that marks the existing concepts within the taxonomy and an overview of those concepts.
- An overview showing all the concepts detected within the document, including newly identified concepts to expand the taxonomy. These are generated in triples, using the SKOS vocabulary.
Both will enable the Dutch National Police to decrease the amount of time required to find and extract new concepts.

At this date of submission, the tooling has already been used by our modellers during the analysation of new regulation documents, further accelerating the selection of new concepts. In further development, the solution will be enriched so that not only regulations and law articles can be analysed, but also other types of documents. The long-term aspiration is to enhance this solution by not only extracting new entities but also automating the construction of new taxonomies.

",,,
5806,4,sem23-industry,How to manage regulatory policies with knowledge graphs to mitigate compliance risks,Ian Eccleston and Irene Polikoff,2023-06-08 17:12,1970-01-01 00:00,(Demo link) ,"knowledge graphs
case study
regulatory compliance",accept,"In this talk we will describe our experience working with a major financial institution to improve their compliance risk management. As part of this effort, we have:

1. modeled policies and regulations
2. captured risk indicators for regulatory compliance
3. cataloged data and technology assets
4. automated mapping of risk indicators to data elements and
5. were able to infer role of data elements in compliance risk assessment.

Managing regulations and policies in knowledge graphs enabled the firm more effectively and accurately identify data elements containing data needed to be assessed for compliance. This repeatable and scalable approach resulted in mitigating regulatory compliance risks.",,,
5822,4,sem23-industry,Why an Event-Native Mindset is Essential for Knowledge Graphs,Jans Aasman and Sheng-Chuan Wu,2023-06-13 22:03,1970-01-01 00:00,(Demo link) ,"Event-Native
Knowledge Graph
Event-driven Architecture
Entity-event
Graphs
Data modeling
Customer 360",accept,"Gartner has noted that it is not enough for businesses today to be “ready to change,” but instead, companies need to be “ready to act” in real-time and understand the context of the action. Simply being prepared to respond to change is not sufficient in today’s fast-paced and constantly evolving business environment. Organizations need to be able to anticipate and proactively respond to changes in order to stay competitive.

But how can a business create a data architecture that supports ‘ready to act’ applications and systems? Gartner suggests embracing an Event–Native Mindset and with an Event-Driven Architecture that delivers continuous intelligence and keeps the business always ready.

Applying an Event–Native Mindset to Data Modeling 

Consider for a moment that everything that happens within a business environment is an event, and every event impacts an entity or is carried out by an entity. An entity in this context is a core business concept like a customer, patient or product. Everything a patient does – getting diagnosed, visiting a specialist, being discharged or receiving a prescription – is an event. Anything that happens to a business’ customer, from making purchases, returns or  calling for support, is an event. When products are created, tested, and updated, these activities are also events.

By adopting an Event–Native Mindset in data modeling, organizations can develop more agile and responsive data architectures that are better suited to handle complex and rapidly changing data environments. This approach can help organizations to more quickly identify and respond to changes in data patterns, and to more effectively leverage the insights and value that can be derived from event-driven data models.

During this presentation we will discuss best practices for building Event-Driven Knowledge Graphs based on several industry projects in Pharmaceuticals, Manufacturing and Consumer Service Centers.
",,,
5847,4,sem23-industry,Use of healthcare ontologies at scale in IQVIA,Andrea Splendiani,2023-06-12 21:47,2023-06-13 22:29,(Demo link) ,"ontology
healthcare
ontology mapping
MDM",accept,"In IQVIA we process and integrate healthcare data at scale (100B+ medical records per year). In our operations, we make a varied use of ontologies: to develop metadata schema to annotate data quality; to standardise terminologies and code lists to enable scalable analytics (e.g.: as per OMOP standard for Real World Evidence); or to standardise data for regulatory submissions (e.g.: as in CDISC for FDA clinical trials submissions). 
In this talk, we would like to showcase some of the peculiarities and learnings of working with ontologies at such scale (note that in the healthcare space, “ontologies” encompass a broad range of artefacts from code lists to DL-based representations).
First, we stratify our ontologies in three different layers:
•	“standard ontologies”: public or not, these are the standards in use, that form the basis for interoperability.
•	“actual ontologies”: as a result of the broad observation of the healthcare system that we perform, we collect a variety of terms in use that don’t directly map to standard ontologies (>44M). These can correspond to different names, language or regional variations, new concepts not yet standardised (e.g.: a newly released covid self-test) or simply concepts not captured by standards (e.g.: laboratory consumables).
•	“curated ontologies”: for many of our analytics need, we need extended reference data that is typically not found in the public space (e.g.: comprehensive normalisation of products across markets, or identification of healthcare organizations).
Standard ontologies pose different order of challenges, like different variations and multiple coexisting versions. Often standard ontologies “in use” are also affected by some amount of noise (e.g.: accidental mix of ontologies to diagnose an indication). We intend to showcase some of the internal tooling that we have developed to address such problems.
Actual ontologies present a view of what of an ontology is practically in use: (e.g.: not all terms of branches of ontologies are used, some probably representing theoretical concepts not existing in clinical practice). A question to foster a discussion with the audience is to what extent the gap between “standard ontologies” (a-priori) or “actual ontologies” (a-posteriori) is at the basis of interoperability issues.
Finally, we note that our “curated ontologies” (missing ontologies we developed to support analytics), rather present information typical of MDM system. We would then conclude with a framework we are using to merge the MDM and the ontology mindsets.

---

I am currently a full time employee (Principal) in IQVIA (An SP500 healthcare data company), focusing on internal optimization and product development in the area of ontology and metadata. I also organise http://swat4ls.org (conference on semantic technologies in life sciences) as a pro-bono activity.
 
In terms of expertise and background:
My PhD thesis was on the use of ontologies, and I have published a few papers on ontologies, semantics and FAIR: https://orcid.org/0000-0002-3201-9617
Before joining IQVIA I was (in reverse order):
Director of data strategy in Novartis (corporate): proposing KGs to link data across the enterprise
Associate director information systems at Novartis: supervising an ontology provision system and working on data curation semi-automation
Independent consultant: semantic POCs for clinical trials, nutrition, meteorology (WHO) and more
Staff scientist: data integration based on ontologies
Postdoc in medical terminologies
I know mostly focus on ontology business cases, technology development, change management and communication.",,,
5860,1,sem23-research,The FLINT ontology: an actor-based model of legal relations,"Jeroen Breteler, Thom Van Gessel, Giulia Biagioni and Robert Van Doesburg",2023-05-10 09:59,2023-05-31 09:51,"(Paper Type) Short
(Student) 
(Publication Resources) Yes
(Code Availability) https://gitlab.com/normativesystems/knowledge-modeling/flint-ontology
(Data Availability) 
(Ontology Availability) 
(Demo link) ","legaltech
norms
normative systems",accept,"Recording and documenting human and AI-driven normative decision-making processes has so far been highly challenging.
We focus on the challenge of normative coordination: The process by which stakeholders in a community understand and agree what norms they abide by.
Our aim is to develop and formalize the FLINT language, which allows a high-level description of normative systems.
FLINT enables legal experts to agree on norms, while also serving as a basis for technical implementation. 

Our contribution consists of the development of an ontology for FLINT and its RDF/OWL implementation which we have made openly accessible. We designed the ontology on the basis of competency questions. Additionally, we validated the ontology by modeling example cases and using the ontology's data model in software tooling.",https://doi.org/10.3233/SSW230016,10.3233/SSW230016,https://2023-eu.semantics.cc/img/illustrations/5860-Semantic23-DrawingText-12.png
6126,4,sem23-industry,Managing the metadata of a diverse digital media archive as a Knowledge Graph,Miel Vander Sande,2023-05-11 07:51,1970-01-01 00:00,(Demo link) ,"knowledge graph
media
audiovisual
archive
data integration",accept,"Meemoo manages a large quantity of mainly audiovisual material from more than 170 partners in cultural heritage and media. More than 6 million objects are currently stored, ranging from digitised newspapers, photos, videos, and audio. In addition, a number of access platforms make the digitised content available to specific target groups, including teachers, students, professional re-users, or the public.

Metadata is a key element in all of meemoo’s processes. An important part of our activities is to collect, integrate, manage, and search a large variety of heterogeneous metadata across the archived content. The scale of this has increased enormously, so a good and integrated approach is needed to deal with the amount of metadata, its need for flexibility, and how easy it is to find. One of the specific challenges is modelling and storing data from machine learning algorithms (speech recognition, face detection and entity recognition) for reuse.

In this talk, we will discuss the key points and lessons learned from implementing the new metadata roadmap at meemoo (https://meemoo.be/en/publications/metadata-roadmap-the-route-to-an-improved-metadata-infrastructure), which is focused on a semantic Knowledge Graph-based infrastructure. The goal of the roadmap is to establish a better data practice within the organization and offer application-independent, uniform access to (meta)data that is spread across various systems and formats. The implementation consists of extensive data modeling, using RDF, SKOS, SHACL to construct a uniform view over all relevant data, and using SPARQL in combination with GraphQL for estabishing efficient access by platforms.
",,,
7519,1,sem23-research,Polyvocal Knowledge Modelling for Ethnographic Heritage Object Provenance,"Sarah Binta Alam Shoilee, Victor de Boer and Jacco van Ossenbruggen",2023-05-08 12:59,2023-05-15 14:40,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://doi.org/10.5281/zenodo.7437713
(Data Availability) https://doi.org/10.5281/zenodo.7437713
(Ontology Availability) 
(Demo link) ","Provenance information
Polyvocality
Domain modelling
Knowledge Representation",accept,"Purpose: Information about biographies of museum object objects (object provenance) is often unavailable in a machine-readable format. This limits the findability and reusability of object provenance information for domain research. We address the challenges of defining a data model to represent ethnographic cultural heritage objects' provenance, which includes multiple interpretations (polyvocality) of, and theories for, the object biography, chains of custody and context of acquiring.

Methodology: To develop a data model for representing the provenance of ethnographic objects, we conducted (semi-)structured interviews with five provenance experts to elicit a set of requirements. Based on these requirements and a careful examination of six diverse examples of ethnographic object provenance reports, we established a set of modelling choices that utilise existing ontologies such as CIDOC-CRM (a domain standard) and PROV-DM, as well as RDF-named graphs.. 

Evaluation: Finally, we validate the model on provenance reports containing six seen and five unseen ethnographic cultural heritage object from three separate sources. The 11 reports are converted into RDF triples following the proposed data model. We also constructed SPARQL queries corresponding to nine competency questions elicited from domain experts in order to report on satisfiability.

Findings: The results show that the adapted combined model allows us to express the heterogeneity and polyvocality of the object provenance information, trace data provenance and link with other data sources for further enrichment. 

Value: The proposed model from this paper allows publishing such knowledge in a machine-readable format, which will foster information contextualisation, findability and reusability.",https://doi.org/10.3233/SSW230010,10.3233/SSW230010,https://2023-eu.semantics.cc/img/illustrations/7519-Semantic23-DrawingText-13.png
7541,4,sem23-industry,From Skills to Service: How Fraunhofer Society's Skill Catalogue Optimizes Research Request Management,"Jan-Peter Bergmann, Elena Senger, Yuri Campbell and Karl Trela",2023-05-29 10:28,1970-01-01 00:00,(Demo link) ,"Knowledge Management
Language Models
Machine Learning
BERT",accept,"Initial Situation: 

Our organization, the Fraunhofer Society, faced a challenge in handling the influx of research requests coming in from various external partners and customers. The task of sorting these requests to match with the relevant researchers within our organization (~22.000) was not streamlined and would either cause an information overload for our researchers or be a time consuming and manual task for their managers. 

Approach: 

To rectify this, we devised a strategy that leverages our existing internal skill catalogue, which is a hierarchical presentation of about 1000 skills and sub-skills that were self-reported in the Fraunhofer Society. The methodology entailed mapping each incoming research request to the corresponding skills within our organization. To achieve this, we used a pre-trained language model which had been fine-tuned for paraphrasing paragraphs. After extracting features from both the incoming texts and the skills in the catalogue, we determined their similarity. We engineered a pipeline that accommodates parameters such as text segments, similarity thresholds, target levels of catalogue hierarchy, and crafted prompts. We tested this pipeline by labeling an already labeled dataset of research requests. 

Business Value and Benefits of the Semantic Solution: 

Our method has proven successful in identifying an optimal set of parameters for labeling incoming research requests. The significant benefit of our approach is its ability to alert relevant employees about incoming research requests that match their skills. This strategy effectively minimizes information overload and ensures employees receive alerts only about relevant details and new research opportunities. 

Prospects and Recommendation: 

We are currently integrating this methodology into an internal application, developed by another organizational unit. The application serves as a platform for our external partners and customers to file their research requests digitally. Harnessing the power of language models and organizational resources like a skills catalogue can significantly improve request management and employee productivity.  ",,,
7956,4,sem23-industry,Streamlining Data Intelligence: A Unified Platform for Managing and Analyzing Process Data in Industrial Settings,Sebastian Gabler,2023-06-12 13:48,1970-01-01 00:00,(Demo link) ,"data integration
semantic search
industry 4.0",accept,"This conference submission presents a cutting-edge platform designed to address the challenges associated with managing and analyzing process and process-analytical data in industrial settings. Such data exists in diverse formats, including instrument data and text data, which makes it crucial to have a unified approach for data access and analysis. Our proposed platform serves as a centralized hub, providing a single point of access for searching, contextualizing, visualizing, and analyzing all types of process data.
The platform incorporates a robust semantic layer and semantic search capabilities, which enable users to effectively navigate and explore the data landscape. By leveraging semantic technologies, the platform goes beyond basic keyword-based search, allowing for more intuitive and context-driven data discovery. This ensures that users can easily locate and access the information they need, regardless of its format or source.
Furthermore, our platform offers both self-service and predefined data objects, catering to the diverse needs of stakeholders across different departments. This flexibility empowers users to generate customized analyses and reports, promoting data-driven decision-making within their respective domains.
To ensure seamless integration and utilization, our platform is carefully instrumented and consumed by stakeholders. We provide a comprehensive overview of the implementation process, highlighting the steps involved in setting up and configuring the platform to align with specific organizational requirements. Additionally, we showcase real-world use cases and success stories from multiple departments, illustrating how the platform has revolutionized their data-driven practices.",,,
8036,1,sem23-research,"Towards a Versatile Terminology Service for Empowering FAIR Research Data: Enabling Ontology Discovery, Design, Curation, and Utilization Across Scientific Communities","Philip Strömert, Vatsal Limbachia, Pooya Oladazimi, Johannes Hunold and Oliver Koepler",2023-05-09 16:12,2023-05-31 08:51,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/TIBHannover/TIB-Terminology-Service-Frontend-2.0
(Data Availability) 
(Ontology Availability) https://github.com/NFDI4Chem/VibrationalSpectroscopyOntology
(Demo link) https://service.tib.eu/terminology
https://terminology.tib.eu
https://terminology.nfdi4chem.de","FAIR Data
Ontology
Terminology Service
Research Data
Chemistry",accept,"Purpose: We present a versatile Terminology Service (TS) designed for discovery and provision of ontologies, but also their design, curation and utilization. Our work is embedded in the roadmap of the National Research Data Infrastructure (NFDI) to support all major steps in research data life cycles by generating FAIR semantic artifacts. To fully unlock the potential of data, it must be made machine-actionable and semantically enriched. We present the TS, its user-driven development in conjunction with a use case of an ontology design process.
Methodology: Utilizing a user-centric methodology, we devised a TS that accommodates diverse use cases from multiple scientific communities. A major focus was on a balanced implementation of service-to-service features and usability of human interfaces providing rich functionalities for all stakeholders. By engaging domain experts, knowledge workers, and ontology engineers, we facilitated a collaborative ontology design process. This involved the application and evaluation of the TS, along with supplementary tools, workflows, and collaboration models.
Findings: We demonstrate the feasibility, prerequisites, and ongoing challenges associated with developing TS that encompass numerous aspects of ontology utilization to produce FAIR, machine-actionable data. A robust API facilitates ontology integration into research data infrastructures, promoting semantic enrichment and data harmonization across various workflows. GUIs must cater to diverse needs, from domain experts to ontology engineers, for ontology design and curation. We observed the demand for tools offering comprehensive views across disciplines and their ontologies, highlighting the importance of harmonization and alignment with common upper-level ontologies.
Value: Ontologies are the cornerstone to generate semantically rich, FAIR research data. TS which support different tasks of ontology design, curation, discovery, provision and utilization across scientific disciplines gain importance. Such services provide unified access to a large number of ontologies fostering their reuse, improvement, and maturation of ontologies. This paper demonstrates the various stakeholders to be considered, their requirements and utilization of a TS with use-cases from ontology curation and design from scientific community perspective.",https://doi.org/10.3233/SSW230005,10.3233/SSW230005,https://2023-eu.semantics.cc/img/illustrations/8036-Semantic23-DrawingText-14.png
8420,2,sem23-pd,Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering,"Lars-Peter Meyer, Johannes Frey, Kurt Junghanns, Felix Brei, Kirill Bulert, Sabine Gründer-Fahrer and Michael Martin",2023-07-04 12:38,2023-07-04 20:19,"(Publication Resources) Yes
(Code Availability) https://github.com/AKSW/LLM-KG-Bench
(Data Availability) https://github.com/AKSW/LLM-KG-Bench-Results/tree/main/2023-Semantics_Developing-Scalable-Benchmark-for-LLM-KGE
(Ontology Availability) 
(Demo link) ","Large Language Model
Knowledge Graph Engineering
Large Language Model Benchmark",accept,"As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering(KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as also statistical data with visualization generation to support tracking of prompt engineering and model performance.",https://ceur-ws.org/Vol-3526/paper-04.pdf,,
8743,4,sem23-industry,Intelligent Service Agent at ZEISS,"Karsten Schrempp, Klaus Müller and Andreas Pawlik",2023-07-04 08:22,1970-01-01 00:00,(Demo link) ,"Machine Learning
Knowledge Graph
Intelligent Content
Predictive Maintenance
Recommendation",accept,"We are presenting the Intelligent Service Agent realized at ZEISS. It's a knowledge graph-based recommendation service to guide service technicians at the point of need. The agent is fed by various data sources from technical documentation, over spare part lists to maintenance reports. The talk will illustrate the project from data gathering, data enrichment, and machine learning to predictive maintenance. ",,,
8777,1,sem23-research,QALD-9-ES: A Spanish Dataset for Question Answering Systems,"Javier Soruco, Diego Collarana, Andreas Both and Ricardo Usbeck",2023-05-22 21:32,2023-07-10 17:49,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/KGQA/QALD_9_plus
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Knowledge Graphs
Question Answering
Dataset",accept,"Knowledge Graph Question Answering (KGQA) systems enable access to semantic information for any user who can compose a question in natural language. KGQA systems are now a core component of many industrial applications, including chatbots and conversational search applications. Although distinct worldwide cultures speak different languages, the number of languages covered by KGQA systems and its resources is mainly limited to English. To implement KGQA systems worldwide, we need to expand the current KGQA resources to languages other than English. Taking into account the recent popularity that Large-Scale Language Models are receiving, we believe that providing quality resources is key to the development of future pipelines. One of these resources is the datasets used to train and test KGQA systems. Among the few multilingual KGQA datasets available, only one covers Spanish, i.e., QALD-9. We reviewed the Spanish translations in the QALD-9 dataset and confirmed several issues that may affect the KGQA system's quality. Taking this into account, we created new Spanish translations for this dataset and reviewed them manually with the help of native speakers.  This dataset provides newly created, high-quality translations for QALD-9; we call this extension QALD-9-ES. We merged these translations into the QALD-9-plus dataset, which provides trustworthy native translations for QALD-9 in nine languages, intending to create one complete source of high-quality translations. We compared the new translations with the QALD-9 original ones using language-agnostic quantitative text analysis measures and found improvements in the results of the new translations. Finally, we compared both translations using the GERBIL QA benchmark framework using a KGQA system that supports Spanish. Although the question-answering scores only improved slightly, we believe that improving the quality of the existing translations will result in better KGQA systems and therefore increase the applicability of KGQA w.r.t. the Spanish language domain.",https://doi.org/10.3233/SSW230004,10.3233/SSW230004,https://2023-eu.semantics.cc/img/illustrations/8777-Semantic23-DrawingText-15.png
9033,2,sem23-pd,A Mapping Lifecycle for Public Procurement Data,"Eugeniu Costetchi, Alexandros Vassiliades and Csongor I. Nyulas",2023-06-20 16:44,1970-01-01 00:00,"(Publication Resources) Yes
(Code Availability) https://github.com/OP-TED/ted-rdf-mapping
(Data Availability) https://github.com/OP-TED/ted-rdf-mapping
(Ontology Availability) https://github.com/OP-TED/ted-rdf-mapping
(Demo link) ","RDF Mapping Language (RML)
Tenders Electronic Daily (TED)
eProcurement Ontology (ePO)
Conceptual Mapping
Technical Mapping",accept,"To ensure the transparency of the public procurement procedures in the European Union (EU), procurement contracts above certain thresholds are governed by EU directives. Every year there are between 700,000 and 1.3 million public procurement related notices published on the Tenders Electronic Daily (TED) website. These notices are created based on the TED Standard Forms, and are published as XML data since 2014. In order to enhance the access and exploration of this data by a larger public, these notices are in the process of being transformed into RDF, conforming to the eProcurement Ontology (ePO). This poster illustrates the framework that we developed to cover the full lifecycle of creating modularised RML mappings to transform Public Procurement Data (PPD) from XML to RDF. The mapping creation lifecycle, has four phases that we repeat for each TED Standard Form: the creation of the conceptual mapping (CM), the creation of the technical mapping (TM), a validation phase, and the dissemination of the mapping. This poster also illustrates our innovative approach of creating reusable CM and TM modules, and automated validation queries, to ensure that our mappings generate a precise and complete RDF representation of the input.",https://ceur-ws.org/Vol-3526/paper-02.pdf,,
9381,4,sem23-industry,From Complexity to Clarity: Configuring Success in Industrial Compatibility Management with Semantic Reasoning and Knowledge Graphs,"Thorsten Liebig, Peter Crocker and Thomas Vout",2023-05-30 13:41,1970-01-01 00:00,(Demo link) ,"Configuration management
Compatibility management
Knowledge graph
Semantic reasoning
Semantic web
AI
rules-based AI
Industry
Graph analytics
Domain expertise
Datalog
OWL
Database
Deployment
Demonstration",accept,"Across various industries and business models, companies face the need to assess compatibility, whether it’s industrial configuration management, terms in contracts, points of a supply chain, buyers and suppliers, and so on.

Whether this process is in relation to assemblies, production, or data analysis, it often requires managing and determining millions of combinations to assess whether components fit together and meet certain requirements. Traditional approaches often struggle with the complexity and scale of real-world scenarios, resulting in lengthy calculation times and restricted insights. Knowledge graphs, on the other hand, perfectly fit these needs and overcome the challenges facing current solutions. In addition to performance, such a semantic solution provides data in the context of domain knowledge, using simple yet expressive axioms and rules instead of disparate hard-coded procedures.

In this presentation, we will showcase a graph solution developed by Derivo for one of the world's largest automation companies, which is currently leveraging this technology in their production processes as well as in an interactive customer-facing configuration solution. The demonstration will highlight the cutting-edge capabilities of RDFox, a knowledge graph and reasoning engine, and how it can be used to streamline compatibility assessment in industrial configuration management. With its in-memory architecture and incremental reasoning, RDFox ensures real-time adaptability and scalability, while also providing increased functionality and analytical power over the industry standards. 

As the complexity of commercial configuration management expands, so too does the need for innovative solutions. Derivo’s partnership with RDFox does just that, propelling us into the future where historical limitations dissolve, giving way to knowledge-based analytics and enhanced decision-making.
",,,
9461,4,sem23-industry,How the Dutch Police deals with context - enforcing compliance while processing ontological data,Paul Brandt and Hans Stolk,2023-06-05 15:24,1970-01-01 00:00,(Demo link) ,"compliance
data provenance
ontology
context of use
design pattern
linked data",accept,"The compliant handling and processing of data by the Dutch National Police is a fundamental requirement for their successful operation. Not only is this required by law, but incorrect processing of data will severely hinder investigation of criminal behaviour, such as the misinterpretation and/or misuse of the available data. The Dutch Police maintains a set of ontologies that can be utilized across the entire police organization. Although this mitigates some of the risks, it is not enough. The true meaning of data is not only dictated by its reference to the ontology, but also its context: at what time, by whom, for what purpose, during what activity. We present ""het verwerkingsmodel"" (""Data Processing Pattern"" in English) that enables us to keep track of data provenance and context in a standardized way. We will demonstrate its practical application at the Dutch Police and discuss how it could be used as a general pattern for any linked data application in which data provenance is important.",,,
9471,1,sem23-research,TRANSRAZ Data Model: Towards a Geosocial Representation of Historical Cities,"Oleksandra Bruns, Tabea Tietz, Sandra Göller and Harald Sack",2023-05-09 17:58,2023-05-23 15:39,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) https://anonymous.4open.science/r/Transraz-2734/; TRANSRAZ data model
(Ontology Availability) https://www.toporaz.de/sparql; SPARQL Endpoint
https://ise-fizkarlsruhe.github.io/Transraz/datamodel; documentation
https://ise-fizkarlsruhe.github.io/Transraz/usecases; use cases and CQs
(Demo link) ","Cultural Heritage
Digital Humanities
City Exploration
Knowledge Graphs
Archival Documents
Architecture",accept,"Preserving historical city architectures and making them publicly available has emerged as an important field of the cultural heritage and digital humanities research domain. In this context, the TRANSRAZ project is creating an interactive 3D environment of the historical city of Nuremberg which spans over different periods of time. Next to the exploration of the city's historical architecture, TRANSRAZ is also integrating information about its inhabitants, organizations, and important events, which are extracted from historical documents semi-automatically. Knowledge Graphs have proven useful and valuable to integrate and enrich these heterogeneous data. However, this task also comes with versatile data modeling challenges. This paper contributes the TRANSRAZ data model, which integrates agents, architectural objects, events, and historical documents into the 3D research environment by means of ontologies. Goal is to explore Nuremberg's multifaceted past in different time layers in the context of its architectural, social, economical, and cultural developments.",https://doi.org/10.3233/SSW230012,10.3233/SSW230012,https://2023-eu.semantics.cc/img/illustrations/9471-Semantic23-DrawingText-16.png
9669,4,sem23-industry,Knowledge Graph based immersive ESG data visualization using metaphoric representations,Sohail El Sabrouty and Ali Khalili,2023-06-04 13:30,2023-06-12 12:05,(Demo link) ,"knowledge graphs
data lineage
business data
virtual reality
immersive 3d visualization
metaphoric representations
esg",accept,"As companies start to focus more on sustainable and responsible business practices in the light of Sustainability Development Goals set by the United Nations, proper documentation of reports, sustainability narratives and policies is therefore of importance. The ESG framework is a framework that enables companies to get insights into their environment, society and governance structures and assess their performance across these three components taking the aforementioned documentation as input.
ESG framework data is often stored in structured formats like spreadsheets and databases, or unstructured formats like text documents or visualizations. However, these formats have limitations. They lack a high-level summary of the ESG framework and fail to depict underlying links between its components, potentially hindering the discovery of meaningful relationships.
To overcome these challenges, we propose an immersive 3D metaphoric representation powered by a graph-based data model in Virtual Reality (VR). This solution incorporates the ESG framework taxonomy into the data model, allowing easy access to specific data points, efficient information retrieval, and extraction of ESG-related data elements. It facilitates data exploration, validation, and comprehensive analysis of ESG data.
Additionally, the immersive 3D metaphoric representation provides a comprehensive and accurate portrayal of data lineage at varying abstract levels within the ESG framework. Users can navigate and explore data lineage in an engaging and intuitive VR environment, enhancing their understanding of the ESG framework and its relationships.",,,
9809,4,sem23-industry,Ontology and Knowledge Graphs for API-Based Search in an Analytics Self-Service App Store,Jannik Wiessler,2023-06-16 16:45,1970-01-01 00:00,"(Demo link) Sehr geehrter Herr Thurner,

nach Rücksprache mit Frau Schmidt von Metaphacts, schicke ich Ihnen nachfolgend ein Abstract als Submission für die Semantics Konferenz im September in Leipzig.","Semantic Search
Self Service Tools
Informed Decision Making",accept,"The transformation towards a data-driven organization has resulted in the rapid growth of analytics self-service tools at Daimler Truck. Discovering and utilizing analytics self-service tools within the app store poses challenges due to the lack of effective search functionality. This presentation introduces a solution that leverages an ontology and knowledge graphs to implement a powerful API-based search in an app store for analytics self-service tools.

We'll begin by explaining the importance of using ontologies in structuring knowledge and explore the use of knowledge graphs as a means to represent and store ontology data. The domain-specific ontology is designed and developed by Daimler Truck captures essential concepts, relationships, and attributes in the analytics self-service tool domain. The knowledge graph integrates app store tools and metadata based on the defined ontology. The implemented search functionality utilizes APIs, allowing users to query the app store based on specific requirements. Semantic reasoning enhances the search results, providing accurate and context-aware tool recommendations.


Finally, we'll discuss the benefits of using ontologies and knowledge graphs for search functionality, including enriched metadata and adaptability for adding new tools and features. Infrastructure and setup will be presented as well.

In conclusion, this presentation showcases the utilization of ontologies and knowledge graphs to implement API-based search in an analytics self-service tools app store. The proposed approach enhances tool discovery, facilitates informed decision-making, and ensures adaptability to the evolving analytics landscape.",,,
